{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHM0OR9jcOfTRUg3rK+22e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harini-siva03/NLP/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNwCCj5LDwWf",
        "outputId": "eb94758d-eead-481b-8cbb-5cd87c0c205a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Method  Unigram Perplexity  Bigram Perplexity\n",
            "0  No Smoothing        96744.663442       1.000000e+06\n",
            "1       Laplace           36.132573       2.165871e+01\n",
            "2         Add-k           45.149348       2.005846e+01\n",
            "3   Good–Turing        96744.663442       3.101908e+00\n",
            "4    Kneser–Ney        96744.663442                NaN\n",
            "5   Witten–Bell        96744.663442                NaN\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import math\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "custom_text = \"\"\"\n",
        "I am Harini.\n",
        "You are my friend.\n",
        "She is a good girl.\n",
        "She is reading a book.\n",
        "It is raining today.\n",
        "We are going to the park.\n",
        "They are playing football.\n",
        "I am learning Python.\n",
        "You are drinking coffee.\n",
        "She is cooking dinner.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    tokens = re.findall(r'\\b[a-z]+\\b', text)\n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess(custom_text)\n",
        "random.seed(42)\n",
        "split_point = int(0.8 * len(tokens))\n",
        "train_tokens = tokens[:split_point]\n",
        "test_tokens = tokens[split_point:]\n",
        "\n",
        "def build_unigram_model(tokens):\n",
        "    counts = Counter(tokens)\n",
        "    total_count = sum(counts.values())\n",
        "    return counts, total_count\n",
        "\n",
        "def build_bigram_model(tokens):\n",
        "    bigram_counts = Counter(zip(tokens[:-1], tokens[1:]))\n",
        "    unigram_counts = Counter(tokens)\n",
        "    return bigram_counts, unigram_counts\n",
        "\n",
        "unigram_counts, total_unigrams = build_unigram_model(train_tokens)\n",
        "bigram_counts, unigram_counts_for_bigram = build_bigram_model(train_tokens)\n",
        "vocab = set(train_tokens)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def perplexity_unigram(test_tokens, prob_func):\n",
        "    N = len(test_tokens)\n",
        "    log_prob_sum = 0\n",
        "    for w in test_tokens:\n",
        "        prob = prob_func(w)\n",
        "        log_prob_sum += math.log(prob)\n",
        "    return math.exp(-log_prob_sum / N)\n",
        "\n",
        "def perplexity_bigram(test_tokens, prob_func):\n",
        "    N = len(test_tokens) - 1\n",
        "    log_prob_sum = 0\n",
        "    for i in range(N):\n",
        "        w1, w2 = test_tokens[i], test_tokens[i+1]\n",
        "        prob = prob_func(w1, w2)\n",
        "        log_prob_sum += math.log(prob)\n",
        "    return math.exp(-log_prob_sum / N)\n",
        "\n",
        "def prob_uni_no(w):\n",
        "    return unigram_counts.get(w, 0) / total_unigrams if unigram_counts.get(w, 0) > 0 else 1e-8\n",
        "\n",
        "def prob_bi_no(w1, w2):\n",
        "    return bigram_counts.get((w1, w2), 0) / unigram_counts_for_bigram[w1] if bigram_counts.get((w1, w2), 0) > 0 else 1e-8\n",
        "\n",
        "def prob_uni_laplace(w):\n",
        "    return (unigram_counts.get(w, 0) + 1) / (total_unigrams + vocab_size)\n",
        "\n",
        "def prob_bi_laplace(w1, w2):\n",
        "    return (bigram_counts.get((w1, w2), 0) + 1) / (unigram_counts_for_bigram[w1] + vocab_size)\n",
        "\n",
        "k_value = 0.5\n",
        "def prob_uni_addk(w):\n",
        "    return (unigram_counts.get(w, 0) + k_value) / (total_unigrams + k_value * vocab_size)\n",
        "\n",
        "def prob_bi_addk(w1, w2):\n",
        "    return (bigram_counts.get((w1, w2), 0) + k_value) / (unigram_counts_for_bigram[w1] + k_value * vocab_size)\n",
        "\n",
        "bigram_count_of_counts = Counter(bigram_counts.values())\n",
        "total_bigrams = sum(bigram_counts.values())\n",
        "def prob_bi_goodturing(w1, w2):\n",
        "    c = bigram_counts.get((w1, w2), 0)\n",
        "    if c < max(bigram_count_of_counts.keys()):\n",
        "        return ((c+1) * (bigram_count_of_counts.get(c+1, 0) / bigram_count_of_counts.get(c, 1))) / total_bigrams\n",
        "    else:\n",
        "        return c / total_bigrams\n",
        "\n",
        "continuation_counts = Counter([w2 for (w1, w2) in bigram_counts.keys()])\n",
        "def prob_bi_kneserney(w1, w2, discount=0.75):\n",
        "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
        "    lambda_w1 = (discount / unigram_counts_for_bigram[w1]) * len([w for (u, w) in bigram_counts if u == w1])\n",
        "    p_continuation = continuation_counts[w2] / sum(continuation_counts.values())\n",
        "    return max(bigram_count - discount, 0) / unigram_counts_for_bigram[w1] + lambda_w1 * p_continuation\n",
        "\n",
        "unique_continuations = defaultdict(int)\n",
        "for (w1, w2) in bigram_counts.keys():\n",
        "    unique_continuations[w1] += 1\n",
        "\n",
        "def prob_bi_wittenbell(w1, w2):\n",
        "    T = unique_continuations[w1]\n",
        "    Z = vocab_size - T\n",
        "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
        "    if bigram_count > 0:\n",
        "        return bigram_count / (unigram_counts_for_bigram[w1] + T)\n",
        "    else:\n",
        "        return T / (Z * (unigram_counts_for_bigram[w1] + T))\n",
        "\n",
        "results = []\n",
        "methods = [\n",
        "    (\"No Smoothing\", prob_uni_no, prob_bi_no),\n",
        "    (\"Laplace\", prob_uni_laplace, prob_bi_laplace),\n",
        "    (\"Add-k\", prob_uni_addk, prob_bi_addk),\n",
        "    (\"Good–Turing\", prob_uni_no, prob_bi_goodturing),\n",
        "    (\"Kneser–Ney\", prob_uni_no, prob_bi_kneserney),\n",
        "    (\"Witten–Bell\", prob_uni_no, prob_bi_wittenbell)\n",
        "]\n",
        "\n",
        "for name, uni_func, bi_func in methods:\n",
        "    try:\n",
        "        pp_uni = perplexity_unigram(test_tokens, uni_func)\n",
        "    except:\n",
        "        pp_uni = None\n",
        "    try:\n",
        "        pp_bi = perplexity_bigram(test_tokens, bi_func)\n",
        "    except:\n",
        "        pp_bi = None\n",
        "    results.append([name, pp_uni, pp_bi])\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"Method\", \"Unigram Perplexity\", \"Bigram Perplexity\"])\n",
        "print(df)"
      ]
    }
  ]
}